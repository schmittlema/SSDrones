{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpXw8Me0\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint, gauss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DiscreteHill(object):\n",
    "    \n",
    "    directions = [(0,1), (0,-1), (1,0), (-1,0)]\n",
    "    \n",
    "    def __init__(self, board=(10,10), variance=4.):\n",
    "        self.variance = variance\n",
    "        self.target = (0, 0)\n",
    "        while self.target == (0, 0):\n",
    "            self.target   = (randint(-board[0], board[0]), randint(-board[1], board[1]))\n",
    "        self.position = (0, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add(p, q):\n",
    "        return (p[0] + q[0], p[1] + q[1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def distance(p, q):\n",
    "        return abs(p[0] - q[0]) + abs(p[1] - q[1])\n",
    "    \n",
    "    def estimate_distance(self, p):\n",
    "        distance = DiscreteHill.distance(self.target, p) - DiscreteHill.distance(self.target, self.position)\n",
    "        return distance + abs(gauss(0, self.variance))\n",
    "        \n",
    "    def observe(self):    \n",
    "        return np.array([self.estimate_distance(DiscreteHill.add(self.position, delta)) \n",
    "                         for delta in DiscreteHill.directions])\n",
    "    \n",
    "    def perform_action(self, action):\n",
    "        self.position = DiscreteHill.add(self.position, DiscreteHill.directions[action])\n",
    "        \n",
    "    def is_over(self):\n",
    "        return self.position == self.target\n",
    "    \n",
    "    def collect_reward(self, action):\n",
    "        return -DiscreteHill.distance(self.target, DiscreteHill.add(self.position, DiscreteHill.directions[action])) \\\n",
    "            + DiscreteHill.distance(self.target, self.position) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_prev_frames = 3\n",
    "\n",
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.python.framework.ops.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([n_prev_frames * 4 + n_prev_frames - 1,], [4], \n",
    "            [tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(n_prev_frames * 4 + n_prev_frames - 1, 4, brain, optimizer, session,\n",
    "                                   discount_rate=0.9, exploration_period=100, max_experience=10000, \n",
    "                                   store_every_nth=1, train_every_nth=4, target_network_update_rate=0.1,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 3100: iterations before success 32. Pos: (4, 0), Target: (4, 0)"
     ]
    }
   ],
   "source": [
    "performances = []\n",
    "\n",
    "try:\n",
    "    for game_idx in range(10000):\n",
    "        game = DiscreteHill()\n",
    "        game_iterations = 0\n",
    "\n",
    "        observation = game.observe()\n",
    "        \n",
    "        prev_frames = [(observation, -1)] * (n_prev_frames - 1)\n",
    "        memory = np.concatenate([np.concatenate([observation, np.array([-1])])] * (n_prev_frames - 1) + [observation])\n",
    "        \n",
    "        while game_iterations < 50 and not game.is_over():\n",
    "            action = current_controller.action(memory)\n",
    "            if n_prev_frames > 1:\n",
    "                prev_frames = prev_frames[1:] + [(observation, action)]\n",
    "            reward = game.collect_reward(action)\n",
    "            game.perform_action(action)\n",
    "            observation = game.observe()\n",
    "            new_memory = np.concatenate([np.concatenate([a, np.array([b])]) for (a, b) in prev_frames] + [observation])\n",
    "            current_controller.store(memory, action, reward, new_memory)\n",
    "            current_controller.training_step()\n",
    "            memory = new_memory\n",
    "            game_iterations += 1\n",
    "            cost = abs(game.target[0]) + abs(game.target[1])\n",
    "        performances.append((game_iterations - cost) / float(cost))\n",
    "        if game_idx % 100 == 0:\n",
    "            print \"\\rGame %d: iterations before success %d.\" % (game_idx, game_iterations),\n",
    "            print \"Pos: %s, Target: %s\" % (game.position, game.target),\n",
    "except KeyboardInterrupt:\n",
    "    print \"Interrupted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "smooth_performances = [float(sum(performances[i:i+N])) / N for i in range(0, len(performances) - N)]\n",
    "\n",
    "plt.plot(range(len(smooth_performances)), smooth_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = brain.layers[0].Ws[0].eval()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.matshow(x)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brain.input_layer.b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.collect_reward(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate([observation, np.array([-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_prev_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performances_1 = performances[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.average(performances[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
